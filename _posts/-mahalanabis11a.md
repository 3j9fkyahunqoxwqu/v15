---
title: Learning mixtures of Gaussians with maximum-a-posteriori oracle
abstract: We consider the problem of estimating the parameters of a mixture of distributions,
  where each component distribution is from a given parametric family e.g. exponential,
  Gaussian etc. We define a learning model in which the learner has access to a ``maximum-a-posteriori''
  oracle which given any sample from a mixture of distributions, tells the learner
  which component distribution was the most likely to have generated it. We describe
  a learning algorithm in this setting which accurately estimates the parameters of
  a mixture of k spherical Gaussians in $R^d$ assuming the component Gaussians satisfy
  a mild separation condition. Our algorithm uses only polynomially many (in d, k)
  samples and oracle calls, and our separation condition is much weaker than those
  required by unsupervised learning algorithms like [Arora 01, Vempala 02]. [pdf]
pdf: "./mahalanabis11a/mahalanabis11a.pdf"
layout: inproceedings
key: mahalanabis11a
month: 0
firstpage: 489
lastpage: 497
origpdf: http://jmlr.org/proceedings/papers/v15/mahalanabis11a/mahalanabis11a.pdf
sections: 
authors:
- given: Satyaki
  family: Mahalanabis
---
