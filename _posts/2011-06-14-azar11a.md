---
title: Dynamic Policy Programming with Function Approximation
abstract: In this paper, we consider the problem of planning   in the infinite-horizon
  discounted-reward Markov decision problems.  We propose a novel iterative method,
  called dynamic policy programming (DPP), which updates the parametrized policy by
  a Bellman-like iteration.  For discrete state-action case, we establish  sup-norm
  loss bounds for the performance of the policy induced by DPP and  prove that it
  asymptotically converges to the optimal policy.  Then, we generalize our approach
  to large-scale (continuous) state-action problems using function approximation technique.   We
  provide  sup-norm performance-loss bounds  for approximate DPP and compare these
  bounds with the standard results  from  approximate dynamic programming (ADP) showing
  that approximate DPP results in a tighter asymptotic bound than standard ADP methods.  We
  also numerically compare the performance of DPP to other ADP and RL methods.  We
  observe that approximate DPP  asymptotically outperforms other methods on the mountain-car
  problem.  [pdf][supplementary]
pdf: http://proceedings.pmlr.press/azar11a/azar11a.pdf
supplementary: Supplementary:http://jmlr.org/proceedings/papers/v15/azar11a/azar11aSupple.pdf
layout: inproceedings
id: azar11a
month: 0
firstpage: 119
lastpage: 127
page: 119-127
origpdf: http://jmlr.org/proceedings/papers/v15/azar11a/azar11a.pdf
sections: 
author:
- given: Mohammad Gheshlaghi
  family: Azar
- given: VicenÃ§
  family: GÃ³mez
- given: Bert
  family: Kappen
date: 2011-06-14
publisher: PMLR
container-title: Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics
volume: '15'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 6
  - 14
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
