---
title: Deep Learners Benefit More from Out-of-Distribution Examples
abstract: Recent theoretical and empirical work in statistical machine learning has
  demonstrated the potential of learning algorithms for deep architectures, i.e.,
  function classes obtained by composing multiple levels of representation. The hypothesis
  evaluated here is that intermediate levels of representation, because they can be
  shared across tasks and examples from different but related distributions, can yield
  even more benefits. Comparative experiments were performed on a large-scale handwritten
  character recognition setting with 62 classes (upper case, lower case, digits),
  using both a multi-task setting and perturbed examples in order to obtain out-of-distribution
  examples. The results agree with the hypothesis, and show that a deep learner did
  {\em beat previously published results and reached human-level performance}. [pdf]
pdf: "./bengio11b/bengio11b.pdf"
layout: inproceedings
id: bengio11b
month: 0
firstpage: 164
lastpage: 172
page: 164-172
origpdf: http://jmlr.org/proceedings/papers/v15/bengio11b/bengio11b.pdf
sections: 
author:
- given: Yoshua
  family: Bengio
- given: FrÃ©dÃ©ric
  family: Bastien
- given: Arnaud
  family: Bergeron
- given: Nicolas
  family: Boulanger–Lewandowski
- given: Thomas
  family: Breuel
- given: Youssouf
  family: Chherawala
- given: Moustapha
  family: Cisse
- given: Myriam
  family: CÃ´tÃ©
- given: Dumitru
  family: Erhan
- given: Jeremy
  family: Eustache
- given: Xavier
  family: Glorot
- given: Xavier
  family: Muller
- given: Sylvain Pannetier
  family: Lebeuf
- given: Razvan
  family: Pascanu
- given: Salah
  family: Rifai
- given: FranÃ§ois
  family: Savard
- given: Guillaume
  family: Sicard
date: '2011-06-14 00:02:44'
publisher: PMLR
---
